{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chonholee/tutorial/blob/main/bigdata/BigdataII_Titanic_beginner_blank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OJHbN6ZseoV"
      },
      "source": [
        "# Titanic for Beginners\n",
        "![](https://i.imgur.com/rRFchA8.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBX2b6hjseoW"
      },
      "source": [
        "## 目次\n",
        "0. ライブラリ・データ読み込み\n",
        "1. データの概観・分析・前処理\n",
        "2. 機械学習モデルの構築・学習\n",
        "3. 予測の出力・提出\n",
        "\n",
        "　このノートブックでは目次のように、データに最低限の整形を施して、機械学習モデルを学習し、予測を出力して提出するまでの流れを確認します。より本格的な取り組み方については、[Titanic Problem: A Complete Guide](https://www.kaggle.com/code/blurredmachine/titanic-survival-a-complete-guide-for-beginners)などを参照してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_Y0JQu4seoX"
      },
      "source": [
        "## 0. ライブラリ・データ読み込み  \n",
        "　まず初めに使用するライブラリを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-Cb59vHArUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4242e496-3aa2-4435-ed83-8236a281bde5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32oFQiaxMVM1"
      },
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cUrIIVoseoX"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame, Series\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFrKB7-Kseoc"
      },
      "source": [
        "　次にpandasのread_csv関数を用いて、分析する訓練データ**train.csv**とテストデータ**test.csv**を読み込みます。  \n",
        "　変数名に与えた**df**は、**DataFrame**を意味しています（変数名は何でも構いません）。テストデータは素直にdf_testと命名したのに対して訓練データはdfとだけ命名したのは、後に説明するホールドアウト法やクロスバリデーションにおいて、さらにdfを擬似的な訓練データdf_trainと擬似的なテストデータdf_validに分割することを見越してのものです。\\\n",
        "※ファイルの読み込み方法は自身の作業場所によって変わります。自分の環境に合わせたコード片方を選択してください。両方または自分の環境外のコードを選択するとエラーが起こるので気をつけて下さい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu0M5pn_umM7"
      },
      "source": [
        "# JupyterLabなどローカルで作業する場合\n",
        "# 読み込むデータが格納されたディレクトリのパス，必要に応じて変更の必要あり\n",
        "#パスを設定しましょう。****は各自データをアップロードしたフォルダの名前です。\n",
        "path = \"/content/drive/My Drive/****/\"\n",
        "\n",
        "df = pd.read_csv(path + 'titanic_train.csv')\n",
        "df_test = pd.read_csv(path + 'titanic_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC_ROkINseof"
      },
      "source": [
        "## 1. データの概観・前処理\n",
        "\n",
        "データから有用な知見を得るために、明確な目標があったほうが良いでしょう。\n",
        "\n",
        "例えば、いくつかの具体的な問いを設定してみます。\n",
        "\n",
        "1. タイタニック号の乗客はどのような人達だったのか？\n",
        "2. それぞれの乗客はどのデッキにいたか？また、それは客室の種類とどのような関係にあったか？\n",
        "3. 乗客は主にどこから来たのか？\n",
        "4. 家族連れか、単身者か？\n",
        "\n",
        "これらの基本的な問いの後に、さらに深くデータ解析を進めます。\n",
        "\n",
        "5. 沈没からの生還者には、どのような要因があったのか？\n",
        "\n",
        "[参照：初めてのKaggle](https://www.tsjshg.info/udemy/Lec56-59.html)\n",
        "\n",
        "### 1.1 データの概観\n",
        "\n",
        "データを見ていく上で、まず初めにデータのサイズを確認してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_sdrlHqseog"
      },
      "source": [
        "#*** here ***\n",
        "print('訓練データのデータ数は{}、変数は{}種類です。'.format(    ))\n",
        "print('テストデータのデータ数は{}、変数は{}種類です'.format(    ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQLixJ-sseok"
      },
      "source": [
        "　訓練データの初めの10データを見てみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEUxNB3Nseok"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwvWOBdYseon"
      },
      "source": [
        "　変数名の一覧を見てみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfhCBSODseoo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLaOhwiEseor"
      },
      "source": [
        "　これらの変数名を、README.ipynbに示された変数の説明と対応付けておきましょう。  \n",
        "\n",
        "変数 |定義 |備考  \n",
        "---|---|---\n",
        "Survived |死亡したかどうか |0 = No, 1 = Yes\n",
        "Pclass |チケットのクラス |1 = 1st, 2 = 2nd, 3 = 3rd\n",
        "Name |名前 |\n",
        "Sex |性別 |\n",
        "Age\t|年齢 |\n",
        "SibSp |乗船していた兄弟姉妹・配偶者の数\t|\n",
        "Parch |乗船していた親・子供の数\t|\n",
        "Ticket |チケット番号\t|\n",
        "Fare |チケット料金\t|\n",
        "Cabin |キャビン番号\t|\n",
        "Embarked |乗船した港\t|C = Cherbourg, Q = Queenstown, S = Southampton\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**補足：よく使う関数**"
      ],
      "metadata": {
        "id": "8dHx0LbAohXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 有効値の数、データの型情報\n"
      ],
      "metadata": {
        "id": "1mPwQwC0pbHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 特定の項目を抽出\n",
        "sub_df = df[['Survived','Pclass','Sex','Age']]\n",
        "sub_df.head(5)"
      ],
      "metadata": {
        "id": "f8oDQIDcoc_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 統計量表示\n"
      ],
      "metadata": {
        "id": "CdxftDBPpu8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 条件指定によるデータ抽出\n"
      ],
      "metadata": {
        "id": "f2DuSb05p-uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ヒストグラム : 分布状況の確認に使用\n"
      ],
      "metadata": {
        "id": "TLt_zT1hqdfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ヒストグラムの重ね合わせ\n",
        "df[df['Survived'] == 0]['Age'].hist(bins=20, alpha=0.3, color='red')\n",
        "df[df['Survived'] == 1]['Age'].hist(bins=20, alpha=0.3, color='blue')"
      ],
      "metadata": {
        "id": "ROT0nnFzqqQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# クロス集計\n"
      ],
      "metadata": {
        "id": "10h0z_GKq4_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# クロス集計結果の棒グラフ化\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
        "pd.crosstab(df['Pclass'], df['Survived']).plot.bar(ax=ax[0])\n",
        "pd.crosstab(df['Survived'], df['Pclass']).plot.bar(ax=ax[1])"
      ],
      "metadata": {
        "id": "FReNpzClq-zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgMr5R_xseor"
      },
      "source": [
        "### 1.2 データの分析\n",
        "　次に**EDA**と呼ばれる作業を行います。EDAとは、**Exploratory Data Analysis**の略で、日本語では**探索的データ分析**と訳されます。EDAでは、データを様々な角度から可視化したり、統計量を見ることで、データの特徴や構造を掴もうと試みます。この工程で得られた知見は機械学習モデルを選ぶ上でも、後に述べる特徴量エンジニアリングにおいても有用です。EDAで得た知見が役立つ理由の一つは、機械学習モデルによって仮定しているデータの特徴が異なることです。EDAによりデータに線型性・独立性・連続性などの特徴が観察できたり、後述の特徴量エンジニアリングでデータを加工することにより顕著な特徴を有した新しいデータを得ることができれば、それに適した機械学習モデルを用いることができます。  \n",
        "　以下に行うEDAは、\"EDA To Prediction (DieTanic)\"というAshwini Swain氏によるKaggle Notebookを参考にしたものです。\n",
        "  \n",
        "EDA To Prediction (DieTanic)：https://www.kaggle.com/ash316/eda-to-prediction-dietanic\n",
        "\n",
        "　まずは欠損値を確認しておきましょう。機械学習を用いたデータ分析に取り組む上で欠損値の確認は必須となっています。なぜならほとんどの機械学習モデルの実装は欠損値を含むデータに対して学習や予測ができず、エラーとなってしまうからです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZAmFJf8seos"
      },
      "source": [
        "# 学習データ\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhCrAHyuseov"
      },
      "source": [
        "# テストデータ\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt_f7E0yseoz"
      },
      "source": [
        "　**Age**、**Fare**, **Cabin**、**Embarked**の値の一部が欠損していることがわかりました。これらには後で対処することとします。  \n",
        "\n",
        "　次に生存者の割合をみてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_survived = len(    )\n",
        "nb_not_survived = len(    )\n",
        "\n",
        "print (\"Survived: %i (%.1f%%)\"%(nb_survived, float(nb_survived)/len(df)*100.0))\n",
        "print (\"Not Survived: %i (%.1f%%)\"%(nb_not_survived, float(nb_not_survived)/len(df)*100.0))\n",
        "print (\"Total: %i\"%len(df))"
      ],
      "metadata": {
        "id": "nihhP71apWly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enNjCnnfseoz"
      },
      "source": [
        "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
        "\n",
        "df['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\n",
        "ax[0].set_title('Survived')\n",
        "ax[0].set_ylabel('')\n",
        "\n",
        "sns.countplot(x='Survived',data=df,ax=ax[1])\n",
        "ax[1].set_title('Survived')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMggyYvCseo2"
      },
      "source": [
        "　生存率は38.4%であることがわかりました。分析対象となるデータには様々ありますが、一つの分類に**均衡データ**/**不均衡データ**というものがあります。不均衡データとは、主に予測対象のラベルの分布が著しく偏ったデータのことであり、病気の陽性/陰性などがその代表例です。不均衡データを分析する際には、データの前処理やモデルの構築、評価指標の選び方など様々な点において注意しなければなりません。しかし今回の予測対象であるPerishedは生存:死亡がおよそ4:6と均衡しているので、そうした心配の必要はありません。  \n",
        "\n",
        "　次にデータの型について見てみましょう。機械学習を用いてデータ分析を行う際には、データの型にも注意が必要です。なぜならほとんどの機械学習モデルの実装はカテゴリカル変数を含むデータに対して学習や予測ができず、エラーとなってしまうからです。  \n",
        "　データの型には大別して**数値データ**と**カテゴリカルデータ**があります。他にも日付・時間データなどがあったり、連続値データ/離散値データの区別があったりしますが、ここでは扱いません。数値データは文字通り数値が格納されたデータであり、カテゴリカルデータは主に文字列によってその分類が示されたデータです。ただしデータが数値であっても、その値の大小や順序が意味を持たない場合にはカテゴリカルデータとして扱う必要がある点には注意が必要です。  \n",
        "　この観点では今回のデータは以下のように分類されます。\n",
        "- 数値データ：Pclass, Age, SibSp, Parch, Fare\n",
        "- カテゴリカルデータ：Name, Sex, Ticket, Embarked\n",
        "\n",
        "　これらのカテゴリカルデータは機械学習モデルで扱えるよう、後で適切に処理します。\n",
        "\n",
        "　ここからは一つ一つの変数について見ていきましょう。ただし、ここではデモンストレーションとして一部しか扱いません。またデータ分析コンペティションでは、必ずしも全てのEDAを自分で一から行う必要はありません。基本的なEDAは多くの場合Kaggle Notebookとして共有されますし、pandas-profilingなどの便利なライブラリを用いれば済んでしまうからです。しかし他の参加者との差別化を図るには、自らEDAで得た知見を活用する必要があります。また実務においてEDAを肩代わりしてくれる人はいません。これらの理由から、やはり自分である程度のEDAをこなせる必要はあるでしょう。\n",
        "\n",
        "　まずは**Pclass**（チケットのクラス）について見ていきます。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.Pclass.value_counts()"
      ],
      "metadata": {
        "id": "opaXypfZqBMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkBVDcOcseo2"
      },
      "source": [
        "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
        "df['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])\n",
        "ax[0].set_title('Number of Passengers By Pclass')\n",
        "ax[0].set_ylabel('Count')\n",
        "sns.countplot(x='Pclass',hue='Survived',data=df,ax=ax[1])\n",
        "ax[1].set_title('Pclass:Perished vs Survived')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2Ioyq_yseo6"
      },
      "source": [
        "　Pclassごとに人数および死亡率が著しく異なっていることが見て取れます。特にPclass=3は人数が圧倒的に多く、死亡率が著しく高いことがわかります。一方でPclass=1は死亡率が非常に低くなっています。Pclassはチケットのクラスでしたから、ここに見た事実は、Pclassの値が小さいほどチケットのグレードが高いことを直ちに示唆しています。他にはどのような知見が得られるか考えてみましょう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "　次に**Sex**（性別）について見てみます。"
      ],
      "metadata": {
        "id": "4XRAiz1SsJi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x='Sex', y='Survived', data=df)"
      ],
      "metadata": {
        "id": "3jJV3lghsKRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tab = pd.crosstab(df['Pclass'], df['Sex'])\n",
        "print (tab)\n",
        "\n",
        "# sum(1) means the sum across axis 1.\n",
        "tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=False)\n",
        "plt.xlabel('Pclass')\n",
        "plt.ylabel('Percentage')"
      ],
      "metadata": {
        "id": "l8VpaCGDsQCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "　次に**Age**（年齢）について見てみます。"
      ],
      "metadata": {
        "id": "CDoeSHSTsKpT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1BrHQvcseo7"
      },
      "source": [
        "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
        "sns.violinplot(x=\"Pclass\",y=\"Age\", hue=\"Survived\", data=df,split=True,ax=ax[0])\n",
        "ax[0].set_title('Pclass and Age vs Survived')\n",
        "ax[0].set_yticks(range(0,110,10))\n",
        "sns.violinplot(x=\"Sex\",y=\"Age\", hue=\"Survived\", data=df,split=True,ax=ax[1])\n",
        "ax[1].set_title('Sex and Age vs Survived')\n",
        "ax[1].set_yticks(range(0,110,10))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjJQ38U5seo-"
      },
      "source": [
        "　このような図を**バイオリン図**と言います。身近なところでは人口推計の男女別年齢分布が似たような図で示されています。この図からどのような知見が得られるでしょうか。最も顕著な傾向の一つは男性の幼年層に見られます。10歳以下の男性は死亡率が著しく低くなっています。この事実はタイタニック号の事故において幼い男の子が優先的に助けられたことを示唆しています。他にはどのような知見が得られるか考えてみましょう。\n",
        "\n",
        "\n",
        "　最後に**相関行列**の**ヒートマップ**を表示してみましょう。相関行列とは各成分に対応する相関係数を並べた行列のことであり、値の大小に応じて色をつけたものをヒートマップと呼びます。この図を表示することによって、変数間の相関の強さを一目で把握することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88JZv7Dqseo-"
      },
      "source": [
        "sns.heatmap(df.corr(),annot=True,cmap='bwr',linewidths=0.2)\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(10,8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkXN5IylsepC"
      },
      "source": [
        "　この図から、**SibSpとParchの値に比較的強い正の相関がある**ことがわかります。SibSpは同乗していた兄弟姉妹・配偶者の数であり、Parchは同乗していた親・子供の数であったので、この事実は理解しやすいでしょう。ここでSibSpの値とParchの値の和をとって「同乗していた家族の人数」という新しい変数を加えるアイデアが得られます。なぜならSibSpとParchという不自然な分類で二つの変数に分割してあるよりも「同乗していた家族の人数」という変数の方が自然である可能性があるからです。\n",
        "\n",
        "　他にも**PclassとFareの値に比較的強い負の相関**が見られます。この事実は、先に見たようにPclassの値が小さいほどチケットのグレードが高いという見立てを補強しています。この見立ては正しいと見ていいでしょう。  \n",
        "\n",
        "　このように相関が強い変数がある場合には注意が必要です。相関の強い変数を機械学習モデルの学習に用いると、一部のモデルでは**多重共線性**という問題が生じます。そのため著しく相関の強い変数がある場合は、その変数のうち一つだけを残して他の変数を削除するといった対策をすることがあります。ここでは相関が強すぎるという程では無いと見て、こうした対策は行いませんが、自分で試してみても良いでしょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAXXAKwusepC"
      },
      "source": [
        "## 1.3 データの前処理\n",
        "　ここでは、機械学習モデルが学習できるようにデータの前処理を行なっていきます。\n",
        "\n",
        "\n",
        "　まずは**欠損値**の補完です。先に見たように**Age**、**Fare**, **Cabin**、**Embarked**の4変数は一部が欠損していました。欠損値の補完には様々な手法があります。平均値や最頻値といった代表値で補完する手法、機械学習モデルで予測して予測値で補完する手法、-9999などの外れ値で補完することによって欠損していたという情報を保持する手法などが挙げられます。\n",
        "  \n",
        "　このチュートリアルでは、欠損値を含む変数を削除します。欠損値のより良い取り扱い方については、他のサイト（[欠損値処理・補完をタイタニックデータを用いながら図解して理解しよう](https://qiita.com/MANGATA/items/9b988d031832230b9c3a)など）で詳しく学べます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jadhwLcsepC"
      },
      "source": [
        "missing_list = ['Age', 'Fare', 'Cabin', 'Embarked']\n",
        "\n",
        "# データの削除\n",
        "df.drop(missing_list, axis=1, inplace=True)\n",
        "df_test.drop(missing_list, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MII6v_JRsepO"
      },
      "source": [
        "　次に**カテゴリカルデータ**を機械学習モデルで扱えるよう処理します。カテゴリカルデータには、**Name**, **Sex**, **Ticket**, **Embarked**がありました。  \n",
        "　ここでも、カテゴリカルデータである変数を削除してしまいましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPGmom85sepQ"
      },
      "source": [
        "category_list = ['Name', 'Sex', 'Ticket']\n",
        "\n",
        "df.drop(category_list, axis=1, inplace=True)\n",
        "df_test.drop(category_list, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**補足：欠損値の補間、カテゴリ変数の変換**"
      ],
      "metadata": {
        "id": "3-vesGxNvhUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = pd.read_csv(path + 'train.csv')\n",
        "\n",
        "# 欠損値の確認\n",
        "temp.isnull().sum()"
      ],
      "metadata": {
        "id": "nWfDCF4Fvlw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp['Age'].head(10)"
      ],
      "metadata": {
        "id": "pe1M5WI4yucq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp['Age'] = temp['Age'].fillna(temp['Age'].mean())\n",
        "temp['Age'].head(10)"
      ],
      "metadata": {
        "id": "6Q0vQmx9ybCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# カテゴリ変数の確認\n",
        "temp['Sex'].head(10)"
      ],
      "metadata": {
        "id": "tPuGjE5izb6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp['Sex'].map({'male':0, 'female':1})[:10]\n",
        "temp['Sex'].head(10)"
      ],
      "metadata": {
        "id": "G2QOvpNBz4UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_FVCJ58sepS"
      },
      "source": [
        "## 2. 機械学習モデルの構築・学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkRxXhsxsepT"
      },
      "source": [
        "　データが整形できたので、このデータを元に機械学習モデルを構築します。ここではロジスティック回帰というモデルを構築します。より本格的なモデル構築については、他のサイト（[KaggleのTitanicでモデルを選別する](https://qiita.com/sudominoru/items/1c21cf4afaf67fda3fee)など）を参照してください。\n",
        "  \n",
        "　まずdfとdf_testを**説明変数**と**目的変数**に分けます。\n",
        "- 説明変数：モデルの学習に使用する変数、今回の問題ではPassengerId, Survived以外の変数\n",
        "- 目的変数：予測対象の変数, 今回の問題ではSurvived  \n",
        "\n",
        "　ここでスライスしたdfとdf_testを.valuesとしてnumpy.ndarray型に変換しているのは、機械学習モデルの実装によってはこの型のデータしか受け付けないからです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MYx8bIfsepT"
      },
      "source": [
        "X =\n",
        "y =\n",
        "\n",
        "X_test ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw01ZPpgsepW"
      },
      "source": [
        "　機械学習モデルにとって最大の障害の一つは**過学習**です。過学習とは機械学習モデルが訓練データを学習する際に、訓練データに対して正しい予測を与えようとするあまり、訓練データにしか良い予測を与えられず、テストデータや他のデータに対して役に立たなくなってしまう現象のことです。\n",
        "\n",
        "　この現象を回避するための手法の一つに**ホールドアウト法**があります。ホールドアウト法では、与えられた訓練データをさらに擬似訓練データと擬似テストデータに分割し、機械学習モデルを擬似訓練データで学習させます。その上で、擬似訓練データに対する予測精度と擬似テストデータに対する予測精度を比較して、二つの値に大きな解離が見られる場合には過学習が発生していると判断し、過学習を抑えるよう修正を加えます。  \n",
        "\n",
        "　今回は7:3で元の訓練データを分割して、擬似訓練データ(X_train, y_train)と擬似テストデータ(X_valid, y_valid)とします。変数名は何でも構いませんが、ここで用いたvalidとはvalidation(検証)の略です。これは擬似テストデータをモデルの予測精度の検証に用いることに由来します。\n",
        "\n",
        "　データの分割には、scikit-learnのtrain_test_split関数を使用しますが、分割はランダムに行われるため、再現性を保つためには乱数生成のシード値を引数random_stateで指定する必要があります。この値を42とする例が海外を中心に散見されるのは、この数字が、有名なSF作品「銀河ヒッチハイク・ガイド」で「生命、宇宙、そして万物についての究極の疑問の答え」とされているからだそうです。\n",
        "\n",
        "　ホールドアウト法の拡張には、**クロスバリデーション**があります。クロスバリデーションについては、[Kaggle Titanic Tutorial](https://www.kaggle.com/code/sashr07/kaggle-titanic-tutorial)で解説しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNBjfm5XsepX"
      },
      "source": [
        "X_train, X_valid, y_train, y_valid ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BEjGnMysepZ"
      },
      "source": [
        "　ロジスティック回帰モデルを作成して、擬似訓練データ(X_train, y_train)を学習させます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAQjfQ66sepa"
      },
      "source": [
        "lr =\n",
        "\n",
        "lr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzF1Lnwusepc"
      },
      "source": [
        "　このモデルによる予測精度の評価を、今回のコンペティションで指定された評価基準である**正解率(accuracy)**で行います。先述したように、擬似訓練データ(X_train, y_train)に対するスコアと擬似テストデータ(X_valid, y_valid)に対するスコアを見ます。これらの値が著しく解離している場合には、**過学習**が発生しているとして修正を行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA-olXitsepd"
      },
      "source": [
        "print('Train Score: {}'.format(round(lr.score(X_train, y_train), 3)))\n",
        "print(' Test Score: {}'.format(round(lr.score(X_valid, y_valid), 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "補足：ランダムフォレストの例"
      ],
      "metadata": {
        "id": "IgTYyo8z-SzU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgkZKP-psepf"
      },
      "source": [
        "## 3. 予測の出力・提出\n",
        "　学習させたロジスティック回帰モデルを用いて、テストデータに対する予測を行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjuHna45sepf"
      },
      "source": [
        "y_pred =\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2rVrWBksepi"
      },
      "source": [
        "　このようにして提出すべき予測値が得られました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMFZBKrKsepi"
      },
      "source": [
        "　最後に得られた予測値を規定の形式に整形して、csvファイルとして出力しましょう。  \n",
        "　まず規定の形式を確認しましょう。以下のようなcsvファイルで提出するよう指示されていました。\n",
        "\n",
        "PassengerID|Survived\n",
        "---|---\n",
        "892|0\n",
        "893|1\n",
        "894|0\n",
        "…|…\n",
        "1307|0\n",
        "1308|0\n",
        "1309|0\n",
        "\n",
        "　また、gender_submission.csvがその例とされていたので、これを確認します。\\\n",
        "※ファイルの読み込み方法は自身の作業場所によって変わります。自分の環境に合わせたコード片方を選択してください。両方または自分の環境外のコードを選択するとエラーが起こるので気をつけて下さい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scX_XB2hVZAx"
      },
      "source": [
        " # JupyterLabなどローカルで作業する場合\n",
        " # 読み込むデータが格納されたディレクトリのパス，必要に応じて変更の必要あり\n",
        " #パスを設定しましょう。****は各自データをアップロードしたフォルダの名前です。\n",
        "path = \"/content/drive/My Drive/******/\"\n",
        "\n",
        "submission = pd.read_csv(path + 'gender_submission.csv')\n",
        "submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NGSokoAsepj"
      },
      "source": [
        "submission.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POMYauIUsepl"
      },
      "source": [
        "　提出ファイルを作成するには、このデータフレームのSurvivedを上書きするのが手っ取り早いでしょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSUGgVZUsepm"
      },
      "source": [
        "submission['Survived'] = y_pred\n",
        "submission.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH16lEYksepo"
      },
      "source": [
        "　これをcsvファイルとして出力すれば、提出ファイルの完成です。\\\n",
        "※csvファイル書き出しの方法は自身の作業場所によって変わります。自分の環境に合わせたコード片方を選択してください。両方または自分の環境外のコードを選択するとエラーが起こります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XqHNitDV2G_"
      },
      "source": [
        "# JupyterLabなどローカルで作業する場合\n",
        "# パスは必要に応じて変更の必要あり\n",
        "submission.to_csv('/content/drive/My Drive/Lecture_BigData/Kaggle-Titanic/submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uabZEf5_sepp"
      },
      "source": [
        "# Google Drive・Google Colaboratoryで作業する場合\n",
        "\n",
        "from google.colab import files\n",
        "# colaboratory上に保存\n",
        "# 保存したcsvファイルはランタイムが終了すると削除されます\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "# colaboratory上に保存したcsvファイルをローカルに保存\n",
        "files.download('submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdKhrvPfseps"
      },
      "source": [
        "**補足：ランダムフォレストを使用した例**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df2 = pd.read_csv(path + 'train.csv')\n",
        "df2_test = pd.read_csv(path + 'test.csv')\n",
        "\n",
        "# 前処理\n",
        "def preprocess(df):\n",
        "    df['Fare'] = df['Fare'].fillna(df['Fare'].mean())\n",
        "    df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
        "    df['Embarked'] = df['Embarked'].fillna('Unknown')\n",
        "    df['Sex'] = df['Sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
        "    df['Embarked'] = df['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2, 'Unknown': 3} ).astype(int)\n",
        "    df = df.drop(['Cabin','Name','PassengerId','Ticket'],axis=1)\n",
        "    return df\n",
        "\n",
        "# 学習\n",
        "def train(df):\n",
        "    train_x = df.drop('Survived', axis=1)\n",
        "    train_y = df.Survived\n",
        "    (train_x, test_x ,train_y, test_y) = train_test_split(train_x, train_y, test_size = 0.33, random_state = 42)\n",
        "\n",
        "    clf = RandomForestClassifier(random_state=0)\n",
        "    clf = clf.fit(train_x, train_y)\n",
        "    pred = clf.predict(test_x)\n",
        "    print(\"Accuracy: \", accuracy_score(pred, test_y))\n",
        "\n",
        "    features = train_x.columns\n",
        "    importances = clf.feature_importances_\n",
        "    indices = np.argsort(importances)\n",
        "    for i in indices[::-1]:\n",
        "        print(\"{:<15} {:f}%\".format(features[i], importances[i]*100))\n",
        "    return clf\n",
        "\n",
        "df2 = preprocess(df2)\n",
        "clf = train(df2)"
      ],
      "metadata": {
        "id": "NE9-h6b15x-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}