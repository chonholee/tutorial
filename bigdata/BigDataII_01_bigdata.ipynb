{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOLcyQemKzQ9ZVzJvKl9Ty",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chonholee/tutorial/blob/main/bigdata/BigDataII_01_bigdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ビッグデータII　第1回"
      ],
      "metadata": {
        "id": "ceHLloN5LZpA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIetHNNLcXla"
      },
      "source": [
        "# Google Driveとの連携\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# 作業フォルダを各自作って指定する\n",
        "%cd \"/content/drive/MyDrive/Lecture_BigData\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UjxyKXXmJuB"
      },
      "source": [
        "# 大規模データの処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il_oEHJQiaxN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 以下データを置くフォルダを好きに作って指定する\n",
        "DIR_NAME = 'dataset_temp/'\n",
        "\n",
        "# 乱数データセットを作成\n",
        "COL_NAME = [str(i) for i in range(0, 5)]\n",
        "\n",
        "testdata = np.random.rand(100, 5)\n",
        "df = pd.DataFrame(testdata, columns=COL_NAME)\n",
        "\n",
        "filename = DIR_NAME + 'testcsv.csv'\n",
        "df.to_csv(filename, index=False) #書き込み\n",
        "df = pd.read_csv(filename, encoding='utf-8') #読み込み\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd-8ISHAouL1"
      },
      "source": [
        "# ファイルまたはフォルダのサイズを表示\n",
        "!du -h dataset_temp/testcsv.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vxFWLYrtOb7"
      },
      "source": [
        "## 0. Chunkに分けてファイルを読み込む\n",
        "\n",
        "chunksizeを指定する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8u5Rg_zoPf4"
      },
      "source": [
        "i = 0\n",
        "# chunkに分けて処理\n",
        "#--- code here ---#\n",
        "for df in pd.read_csv(filename, encoding='utf-8', xxx):\n",
        "    print(df.shape)\n",
        "    print(df)\n",
        "\n",
        "    # chunkごとに分けて処理\n",
        "    df['5'] = 'chunk ' + str(i)\n",
        "    df.to_csv(DIR_NAME+'testcsv_processed.csv', mode='a', index=False, header=(i == 0))\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOJCFu3oicSL"
      },
      "source": [
        "# データの準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOkxZkWAcVaC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "DIR_NAME = 'dataset_temp/'\n",
        "\n",
        "#乱数作成\n",
        "for i in range(100):\n",
        "    testdata = np.random.rand(100,100)      # 0〜1の乱数で 100x100 の行列を生成\n",
        "    df = pd.DataFrame(testdata)             #dataframeに変換\n",
        "    filename = DIR_NAME + 'dammydata' + str(i).zfill(5) + '.csv'\n",
        "    df.to_csv(filename , index=False)       #csvに保存"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPCdEUFNgh0W"
      },
      "source": [
        "! du -h dataset_temp/dammy*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZuPE1rSeczl"
      },
      "source": [
        "# 1.   For loopで読み込む　pandas.read_csv forloop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkQEbn59feDt"
      },
      "source": [
        "#pandas.read_csv map\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def readcsv_for_loop(fileslist):\n",
        "    for i, file in enumerate(fileslist):\n",
        "        df_tmp = pd.read_csv(file)\n",
        "\n",
        "        if i == 0:\n",
        "            df = df_tmp\n",
        "        else:\n",
        "            # 結合\n",
        "            #---code here---#\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    allfiles = sorted(glob.glob(DIR_NAME+'dammy*.csv', recursive=True))\n",
        "\n",
        "    start = time.time()\n",
        "    df = readcsv_for_loop(allfiles)\n",
        "\n",
        "    process_time = time.time() - start\n",
        "    print('csv読み込み時間：{:.3f}s'.format(process_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqvg8PAUuWQg"
      },
      "source": [
        "# 2．Mapを使って読み込む"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS7lrifHuXl6"
      },
      "source": [
        "# map のサンプル\n",
        "# 参照：https://qiita.com/conf8o/items/0cb02bc504b51af09099\n",
        "\n",
        "data = [1,2,3,4,5]\n",
        "\n",
        "def double(x):\n",
        "  return x*x\n",
        "\n",
        "print(double(data[2]))\n",
        "\n",
        "m = map(double, data)\n",
        "l = list(m)\n",
        "print(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Eb95CihueGu"
      },
      "source": [
        "#pandas.read_csv map\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#mapを利用\n",
        "def read_csv_map(fileslist):\n",
        "    #---code here---#\n",
        "    m =\n",
        "    df = pd.concat(m)\n",
        "    return df\n",
        "\n",
        "#csv1個読み込み(map関数用)\n",
        "def pdreadcsv(csv_path):\n",
        "    return pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    allfiles = sorted(glob.glob(DIR_NAME+'dammy*.csv', recursive=True))\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    df = read_csv_map(allfiles)\n",
        "\n",
        "    process_time = time.time() - start\n",
        "    print('csv読み込み時間：{:.3f}s'.format(process_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBZjjggSxhfZ"
      },
      "source": [
        "# マルチプロセスの例\n",
        "\n",
        "メモ\n",
        "\n",
        "process は、複数の関数を複数プロセスで並列して実行します。実行中の関数は全てメモリ上に展開されます。\n",
        "\n",
        "pool は、一つの関数に複数の処理を行わせる際に、その処理を複数プロセスに割り当てて並列して実行します。pool 側でタスクの分割や結果の統合といったことを暗黙的に制御し、実行中の処理のみがメモリ上に展開されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nnt6VYGxnLU"
      },
      "source": [
        "import time\n",
        "\n",
        "def sum_cube(num):\n",
        "    s = 0\n",
        "    for i in range(num):\n",
        "        s += i * i * i\n",
        "    return s\n",
        "\n",
        "def return_list_sum_cube(numbers):\n",
        "    start = time.time()\n",
        "    result = []\n",
        "    for i in numbers:\n",
        "        result.append(sum_cube(i))\n",
        "\n",
        "    end = time.time() - start\n",
        "    print(f'No Multiprocessing:  {end} seocnds')\n",
        "\n",
        "    return result\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    numbers = range(10)\n",
        "    results = return_list_sum_cube(numbers)\n",
        "    print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqh43iLGxgyP"
      },
      "source": [
        "import time\n",
        "import multiprocessing\n",
        "\n",
        "def sum_cube(num):\n",
        "    s = 0\n",
        "    for i in range(num):\n",
        "        s += i * i * i\n",
        "    return s\n",
        "\n",
        "def return_list_sum_cube_with_multiprocessing(numbers):\n",
        "    start = time.time()\n",
        "\n",
        "    #---code here---#\n",
        "    p =\n",
        "    result =\n",
        "\n",
        "    p.close()\n",
        "    p.join()\n",
        "\n",
        "    end = time.time() - start\n",
        "    print(f'Multiprocessing:  {end} seocnds')\n",
        "\n",
        "    return result\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    numbers = range(10)\n",
        "    result = return_list_sum_cube_with_multiprocessing(numbers)\n",
        "    print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imhEbpI2gRnp"
      },
      "source": [
        "# 3. マルチプロセスでpandas.read_csvをmapで実行してpandas.concatで結合"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBBPRn6_gDgM"
      },
      "source": [
        "#pandas.read_csv map multiprocessing\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from multiprocessing import Pool\n",
        "\n",
        "#map_multiprocessing(pd.concat)\n",
        "def read_csv_map_multi(fileslist):\n",
        "    #---code here---#\n",
        "    p =\n",
        "    df =\n",
        "    p.close()\n",
        "    return df\n",
        "\n",
        "#csv1個読み込み(map関数用)\n",
        "def pdreadcsv(csv_path):\n",
        "    return pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    allfiles = sorted(glob.glob(DIR_NAME+'dammy*.csv', recursive=True))\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    df = read_csv_map_multi(allfiles)\n",
        "\n",
        "    process_time = time.time() - start\n",
        "    print('csv読み込み時間：{:.3f}s'.format(process_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsPwyWyiiBub"
      },
      "source": [
        "# 4. マルチプロセスでpandas.read_csvをmapで実行してnumpy.vstackで結合"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39xE6PDJgWcE"
      },
      "source": [
        "#readcsv_pandas_np.vstack map multi\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from multiprocessing import Pool\n",
        "\n",
        "#map_multiprocessing(np.vstack)\n",
        "def read_csv_map_multi_npvstack(fileslist):\n",
        "    #---code here---#\n",
        "    p =\n",
        "    comb_np_array =\n",
        "    df = pd.DataFrame(comb_np_array)\n",
        "    p.close()\n",
        "\n",
        "    return df\n",
        "\n",
        "def pdreadcsv_np_array(csv_path):\n",
        "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
        "    np_array = df.values\n",
        "    return np_array\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    allfiles = sorted(glob.glob(DIR_NAME+'dammy*.csv', recursive=True))\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    df = read_csv_map_multi_npvstack(allfiles)\n",
        "\n",
        "    process_time = time.time() - start\n",
        "    print('csv読み込み時間：{:.3f}s'.format(process_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khxrX1zVoF1z"
      },
      "source": [
        "大量のcsvファイルを高速に読み込む方法を検討しました。\n",
        "\n",
        "今回紹介した方法が必ずしもベストではなく、csvファイルのサイズとファイル数によって読み込み速度は異なってきます。\n",
        "\n",
        "単純なfor文は想定通り遅く、mapやリスト内包表記を使用することで速度アップができました。\n",
        "\n",
        "また並列処理することでより高速に読み込むことができました。ただし並列処理はＣＰＵ使用率が大幅ＵＰするデメリットもあるので注意が必要です。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DASK"
      ],
      "metadata": {
        "id": "CylOZJf8cWVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "並列処理や分散処理を行ったり、機械学習ライブラリー（Scikit-Learnなど）を高速化することが出来ます。\n",
        "\n",
        "NumPyやPandasは、基本的にシングルコアでの処理を行うため速度が遅くなったり、そもそもデータがメモリに乗らず扱えなかったりします。\n",
        "\n",
        "データ量の大きなデータセットに対し、例えばDaskは並列処理などを駆使し全てのデータに対し処理を行うことができます。\n",
        "\n",
        "ちなみに、並列（Parallel）処理とは、AとBという処理がある場合、同時に処理を行うことです。一方、分散（Distributed）処理は、処理AとBを異なる場所で行うことです。"
      ],
      "metadata": {
        "id": "B9glLFaLcZLg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfX2h45Vh8zb"
      },
      "source": [
        "# 基本ライブラリー読み込み\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# dask関連のライブラリー読み込み\n",
        "import dask\n",
        "import dask.array as da\n",
        "import dask.dataframe as dd\n",
        "! pip install dask_xgboost dask_ml  #インストールされていなければインストール\n",
        "import dask_xgboost                                        #xgboost分類器\n",
        "import xgboost                                             #xgboost分類器\n",
        "import joblib                                              #並列処理（daskと連携）\n",
        "from dask.distributed import Client, progress              #クライアント（並列・分散処理）\n",
        "from dask_ml.datasets import make_classification as dm_mc  #分類問題用のサンプルデータ生成\n",
        "from dask_ml.model_selection import train_test_split       #学習データとテストデータの分割\n",
        "from dask_ml.linear_model import LogisticRegression        #ロジスティック回帰\n",
        "from dask_ml.metrics import accuracy_score                 #分離問題の正答率スコア\n",
        "\n",
        "# sklearnのライブラリー読み込み\n",
        "from sklearn.svm import SVC                                #SVM分類器\n",
        "from sklearn.model_selection import GridSearchCV           #グリッドサーチ\n",
        "from sklearn.datasets import make_classification as sk_mc  #分類問題用のサンプルデータ生成\n",
        "# グラフ描写の設定\n",
        "import matplotlib.pyplot as plt         #ライブラリー読み込み\n",
        "plt.style.use('ggplot')                  #グラフのスタイル\n",
        "plt.rcParams['figure.figsize'] = [12, 9] #グラフサイズ設定"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.uniform(size=(1000, 1000))\n",
        "X"
      ],
      "metadata": {
        "id": "WCHAQ7macso9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# エラーになる numpy: the array size is too big\n",
        "X = np.random.uniform(size=(100000, 100000))\n",
        "X"
      ],
      "metadata": {
        "id": "84JH3I9sc9QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = da.random.random((100000, 100000))\n",
        "X"
      ],
      "metadata": {
        "id": "cswEhdu6dpEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "出力されるアウトプットが、NumPyのときと異なり概要だけになります。\n",
        "\n",
        "Arrayの右にあるChunkが鍵を握っています。DaskのArray（行列）は、Chunkに記載されているチャンクサイズ単位に分割され、分割された行列はNumPyのArray（行列）です。\n",
        "\n",
        "要は、DaskのArray（行列）は、複数のNumPyのArray（行列）で構成されています。\n",
        "\n",
        "チャンクサイズ単位は、指定することができます。"
      ],
      "metadata": {
        "id": "dFFSNnlWeMQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = da.random.random((100000, 100000),chunks=(1000, 1000))\n",
        "X"
      ],
      "metadata": {
        "id": "e_8qLmmVd02h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "行列の演算も同じようにできる\n",
        "\n",
        "compute() : 結果を出力\n",
        "\n",
        "visualize() : 処理を可視化"
      ],
      "metadata": {
        "id": "vGh9Y18t1jyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = da.random.uniform(size=(1000, 1000))\n",
        "X2 = da.random.uniform(size=(1000, 1000))\n",
        "X3 = da.random.uniform(size=(1000, 1000))"
      ],
      "metadata": {
        "id": "JKyWVX-ReQlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = X1 + X2 + X3\n",
        "Y"
      ],
      "metadata": {
        "id": "RvksYc1TecCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "Y.compute()"
      ],
      "metadata": {
        "id": "JBBrwmQIedPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y.visualize()"
      ],
      "metadata": {
        "id": "1MYi5zybed1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "サイズが大きい場合（Chunkごとに並列演算が行われる）"
      ],
      "metadata": {
        "id": "t6p09JSa5YRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = da.random.uniform(size=(5000, 5000))\n",
        "X2 = da.random.uniform(size=(5000, 5000))\n",
        "X3 = da.random.uniform(size=(5000, 5000))"
      ],
      "metadata": {
        "id": "hj0PI1oz5Pef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = X1 + X2 + X3\n",
        "Y"
      ],
      "metadata": {
        "id": "_nzc1xEGeuf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "Y.compute()"
      ],
      "metadata": {
        "id": "x-y7_wnme0n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y.visualize()"
      ],
      "metadata": {
        "id": "-6Butr7Ke4do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "複雑な演算"
      ],
      "metadata": {
        "id": "0wK-2TCF9Sk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = da.random.uniform(size=(5000, 5000))\n",
        "X2 = da.random.uniform(size=(5000, 5000))\n",
        "X3 = da.random.uniform(size=(5000, 5000))\n",
        "\n",
        "X4 = X1 + X2            #行列の和\n",
        "Y = np.dot(X4, X3)      #行列の積\n",
        "invY = np.linalg.inv(Y) #行列の逆行列"
      ],
      "metadata": {
        "id": "XBCqNiMp7SgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "invY.compute()"
      ],
      "metadata": {
        "id": "vm5dhr5p7X56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invY.visualize()"
      ],
      "metadata": {
        "id": "iOXNqNxA-cJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "chankに分けて並列計算"
      ],
      "metadata": {
        "id": "F_NUJwzY-cyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = da.random.uniform(size=(5000, 5000), chunks=(2500,2500))\n",
        "X2 = da.random.uniform(size=(5000, 5000), chunks=(2500,2500))\n",
        "X3 = da.random.uniform(size=(5000, 5000), chunks=(2500,2500))\n",
        "\n",
        "X4 = X1 + X2            #行列の和\n",
        "Y = np.dot(X4, X3)      #行列の積\n",
        "invY = np.linalg.inv(Y) #行列の逆行列"
      ],
      "metadata": {
        "id": "Ld6eET595z-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "invY.compute()"
      ],
      "metadata": {
        "id": "REmiBIyV57D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invY.visualize()"
      ],
      "metadata": {
        "id": "L0RhXGmV7jZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dask DataFrame"
      ],
      "metadata": {
        "id": "HXIxpjImfOOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DaskのDataFrameは、DaskのArray（行列）が多くのNumPy Array（行列）で構成されるのと同様、多くのPandasのDataFrameで構成されます。\n",
        "\n",
        "DaskのDataFrameの演算などは、PandasのDataFrameの演算とほぼ同じです。"
      ],
      "metadata": {
        "id": "scplG-kAfNVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = dask.datasets.timeseries()"
      ],
      "metadata": {
        "id": "t9Vu5YiRfmT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "WG_OwU-I_d0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"x\", \"y\"]].resample(\"1h\").mean().compute().head()"
      ],
      "metadata": {
        "id": "Xs4q_fj2_exq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['x', 'y']].resample('24h').mean().compute().plot()"
      ],
      "metadata": {
        "id": "asdG2Dj9_n58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc['2000-01-05'].compute()"
      ],
      "metadata": {
        "id": "RyIIMaZX_sSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセット（CSV形式）読み込み\n",
        "# Peyton ManningのWikipediaのPV\n",
        "url = 'https://www.salesanalytics.co.jp/bgr8'\n",
        "df = dd.read_csv(url)"
      ],
      "metadata": {
        "id": "rA-DdVFNe6oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "5wg7oXsMfVsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Peyton ManningのWikipediaのPVのプロット\n",
        "df.compute().plot()\n",
        "plt.title('Page Views of Peyton Manning') #グラフタイトル\n",
        "plt.ylabel('Daily Page Views')            #タテ軸のラベル\n",
        "plt.xlabel('Day')                         #ヨコ軸のラベル\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YMoMJsQLfXID"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}