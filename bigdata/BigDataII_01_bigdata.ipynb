{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BigDataII_01_bigdata.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkudRKtv1PIddwUyEHDdHl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chonholee/tutorial/blob/main/bigdata/BigDataII_01_bigdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIetHNNLcXla"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "#　以下好きなフォルダを作って指定する\n",
        "%cd \"/content/drive/MyDrive/Lecture_BigData\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UjxyKXXmJuB"
      },
      "source": [
        "# 大規模データの処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il_oEHJQiaxN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 以下データを置くフォルダを好きに作って指定する\n",
        "DIR_NAME = 'dataset_temp/'\n",
        "\n",
        "COL_NAME = [str(i) for i in range(0,5)]\n",
        "\n",
        "testdata = np.random.rand(100, 5)\n",
        "df = pd.DataFrame(testdata, columns=COL_NAME)\n",
        "filename = DIR_NAME + 'testcsv.csv'\n",
        "df.to_csv(filename, index=False)\n",
        "\n",
        "df = pd.read_csv(filename, encoding='utf-8')\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd-8ISHAouL1"
      },
      "source": [
        "# ファイルまたはフォルダのサイズを表示\n",
        "!du -h dataset_temp/testcsv.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vxFWLYrtOb7"
      },
      "source": [
        "## 0. Chunkに分けてファイルを読み込む"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8u5Rg_zoPf4"
      },
      "source": [
        "i = 0\n",
        "# chunkに分けて処理\n",
        "for df in pd.read_csv(filename, encoding='shift-jis', chunksize=25):\n",
        "    print(df.shape)\n",
        "    print(df)\n",
        "    \n",
        "    # chunkごとに分けて処理\n",
        "    df['5'] = 'chunk ' + str(i)\n",
        "    df.to_csv(DIR_NAME+'testcsv_processed.csv', mode='a', index=False, header=(i == 0))\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOJCFu3oicSL"
      },
      "source": [
        "# データの準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOkxZkWAcVaC"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "DIR_NAME = 'dataset_temp/'\n",
        "\n",
        "#乱数作成\n",
        "for i in range(100):\n",
        "    testdata = np.random.rand(100,100)      # 0〜1の乱数で 100x100 の行列を生成\n",
        "    df = pd.DataFrame(testdata)             #dataframeに変換\n",
        "    filename = DIR_NAME + 'dammydata' + str(i).zfill(5) + '.csv'\n",
        "    df.to_csv(filename , index=False)       #csvに保存"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPCdEUFNgh0W"
      },
      "source": [
        "! du -h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZuPE1rSeczl"
      },
      "source": [
        "# 1.   For loopで読み込む　pandas.read_csv forloop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkQEbn59feDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254af134-d7fa-4c30-9f50-df36f58ab00d"
      },
      "source": [
        "#pandas.read_csv map\n",
        "import time\n",
        "import glob\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "#map(pd.concat)\n",
        "def read_csv_map(fileslist):\n",
        "    df = pd.concat(map(pdreadcsv, fileslist))\n",
        "    return df\n",
        "\n",
        "#csv1個読み込み(map関数用)\n",
        "def pdreadcsv(csv_path):\n",
        "    return pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    allfiles = sorted(glob.glob(DIR_NAME+'*.csv', recursive=True))\n",
        "    \n",
        "    start = time.time()\n",
        "\n",
        "    df = read_csv_map(allfiles)\n",
        "\n",
        "    process_time = time.time() - start\n",
        "    print('csv読み込み時間：{:.3f}s'.format(process_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "csv読み込み時間：1.075s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqvg8PAUuWQg"
      },
      "source": [
        "# 2．Mapを使って読み込む"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS7lrifHuXl6"
      },
      "source": [
        "# map のサンプル\n",
        "# 参照：https://qiita.com/conf8o/items/0cb02bc504b51af09099\n",
        "\n",
        "data = [1,2,3,4,5]\n",
        "\n",
        "def double(x):\n",
        "  return x*x\n",
        "\n",
        "print(double(data[2]))\n",
        "\n",
        "m = map(double, data)\n",
        "l = list(m)\n",
        "print(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Eb95CihueGu"
      },
      "source": [
        "#pandas.read_csv map\n",
        "import time\n",
        "import glob\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "#map(pd.concat)\n",
        "def read_csv_map(fileslist):\n",
        "    #---code here---#\n",
        "    m = map(pdreadcsv, fileslist)\n",
        "    df = pd.concat(m)\n",
        "    return df\n",
        "\n",
        "#csv1個読み込み(map関数用)\n",
        "def pdreadcsv(csv_path):\n",
        "    return pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    allfiles = sorted(glob.glob(DIR_NAME+'*.csv', recursive=True))\n",
        "    \n",
        "    start = time.time()\n",
        "\n",
        "    df = read_csv_map(allfiles)\n",
        "\n",
        "    process_time = time.time() - start\n",
        "    print('csv読み込み時間：{:.3f}s'.format(process_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBZjjggSxhfZ"
      },
      "source": [
        "# マルチプロセスの例\n",
        "\n",
        "メモ\n",
        "\n",
        "process は、複数の関数を複数プロセスで並列して実行します。実行中の関数は全てメモリ上に展開されます。\n",
        "\n",
        "pool は、一つの関数に複数の処理を行わせる際に、その処理を複数プロセスに割り当てて並列して実行します。pool 側でタスクの分割や結果の統合といったことを暗黙的に制御し、実行中の処理のみがメモリ上に展開されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nnt6VYGxnLU"
      },
      "source": [
        "import time \n",
        "\n",
        "def sum_cube(num):\n",
        "    s = 0\n",
        "    for i in range(num):\n",
        "        s += i * i * i\n",
        "    return s\n",
        "\n",
        "def return_list_sum_cube(numbers):\n",
        "    start = time.time()\n",
        "    result = []\n",
        "    for i in numbers:\n",
        "        result.append(sum_cube(i))\n",
        "\n",
        "    end = time.time() - start\n",
        "    print(f'No Multiprocessing:  {end} seocnds')\n",
        "\n",
        "    return result\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    numbers = range(10)\n",
        "    results = return_list_sum_cube(numbers)\n",
        "    print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqh43iLGxgyP"
      },
      "source": [
        "import time \n",
        "import multiprocessing \n",
        "\n",
        "def sum_cube(num):\n",
        "    s = 0\n",
        "    for i in range(num):\n",
        "        s += i * i * i\n",
        "    return s\n",
        "\n",
        "def return_list_sum_cube_with_multiprocessing(numbers):\n",
        "    start = time.time()\n",
        "    \n",
        "    #---code here---#\n",
        "    p = multiprocessing.Pool()\n",
        "    result = p.map(sum_cube, numbers)\n",
        "\n",
        "    p.close()\n",
        "    p.join()\n",
        "\n",
        "    end = time.time() - start\n",
        "    print(f'Multiprocessing:  {end} seocnds')\n",
        "\n",
        "    return result\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    numbers = range(10)\n",
        "    result = return_list_sum_cube_with_multiprocessing(numbers)\n",
        "    print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imhEbpI2gRnp"
      },
      "source": [
        "# 3. マルチプロセスでpandas.read_csvをmapで実行してpandas.concatで結合"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBBPRn6_gDgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeffc390-3e82-408f-95a4-7e48b1dd8efb"
      },
      "source": [
        "#pandas.read_csv map multiprocessing\n",
        "import time\n",
        "import glob\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "from multiprocessing import Pool\n",
        "\n",
        "#map_multiprocessing(pd.concat)\n",
        "def read_csv_map_multi(fileslist):\n",
        "    p = Pool(os.cpu_count())\n",
        "    df = pd.concat(p.map(pdreadcsv, fileslist))\n",
        "    p.close()\n",
        "    return df\n",
        "\n",
        "\n",
        "#csv1個読み込み(map関数用)\n",
        "def pdreadcsv(csv_path):\n",
        "    return pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  \n",
        "    allfiles = sorted(glob.glob(DIR_NAME+'dammy*.csv', recursive=True))\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    df = read_csv_map_multi(allfiles)\n",
        "\n",
        "    process_time = time.time() - start\n",
        "    print('csv読み込み時間：{:.3f}s'.format(process_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "csv読み込み時間：0.853s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsPwyWyiiBub"
      },
      "source": [
        "# 4. マルチプロセスでpandas.read_csvをmapで実行してnumpy.vstackで結合"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39xE6PDJgWcE"
      },
      "source": [
        "#readcsv_pandas_np.vstack map multi\n",
        "import time\n",
        "import glob\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "from multiprocessing import Pool\n",
        "\n",
        "#map_multiprocessing(np.vstack)\n",
        "def read_csv_map_multi_npvstack(fileslist):\n",
        "    p = Pool(os.cpu_count())\n",
        "    comb_np_array = np.vstack(p.map(pdreadcsv_np_array, fileslist))\n",
        "    df = pd.DataFrame(comb_np_array)    \n",
        "    p.close()\n",
        "    \n",
        "    return df\n",
        "\n",
        "def pdreadcsv_np_array(csv_path):\n",
        "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
        "    np_array = df.values\n",
        "    return np_array \n",
        "  \n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    allfiles = sorted(glob.glob(DIR_NAME+'dammy*.csv', recursive=True))  \n",
        "    \n",
        "    start = time.time()\n",
        "\n",
        "    df = read_csv_map_multi_npvstack(allfiles)\n",
        "\n",
        "    process_time = time.time() - start\n",
        "    print('csv読み込み時間：{:.3f}s'.format(process_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khxrX1zVoF1z"
      },
      "source": [
        "大量のcsvファイルを高速に読み込む方法を検討しました。\n",
        "\n",
        "今回紹介した方法が必ずしもベストではなく、csvファイルのサイズとファイル数によって読み込み速度は異なってきます。\n",
        "\n",
        "単純なfor文は想定通り遅く、mapやリスト内包表記を使用することで速度アップができました。\n",
        "\n",
        "また並列処理することでより高速に読み込むことができました。ただし並列処理はＣＰＵ使用率が大幅ＵＰするデメリットもあるので注意が必要です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfX2h45Vh8zb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}